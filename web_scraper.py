# -*- coding: utf-8 -*-
"""Web Scraper.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1plviA7moOxCZiXAwy3I3fBY04XD-iQID
"""

!pip install requests bs4

import csv
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse

base_url = 'https://www.nu.edu.pk/'
visited_links = set()
output_data = []

def extract_data(url):
    try:
        response = requests.get(url, timeout=5)  # Set a timeout value of 5 seconds
        if response.status_code == 200:
            soup = BeautifulSoup(response.content, 'html.parser')
            title = soup.find('title').get_text()
            text = ' '.join([p.get_text() for p in soup.find_all('p')])

            return title, text
        else:
            print(f"Error accessing {url}: Status Code {response.status_code}")
    except requests.exceptions.RequestException as e:
        print(f"Error accessing {url}: {e}")

    return None, None

def clean_url(url):
    # Remove any query parameters and fragments from the URL
    parsed_url = urlparse(url)
    cleaned_url = f"{parsed_url.scheme}://{parsed_url.netloc}{parsed_url.path}"
    return cleaned_url

# def web_crawler(url, depth=2):
#     if depth <= 0 or url in visited_links:
#         return

#     visited_links.add(url)
#     title, text, campus_locations, courses = extract_data(url)
#     if title and text:
#         output_data.append([title, text, campus_locations, courses])

#     try:
#         response = requests.get(url)
#         if response.status_code == 200:
#             soup = BeautifulSoup(response.content, 'html.parser')
#             links = soup.find_all('a', href=True)

#             for link in links:
#                 next_url = link['href']
#                 if next_url.startswith('/'):
#                     next_url = base_url + next_url
#                 web_crawler(next_url, depth - 1)
#     except Exception as e:
#         print(f"Error accessing {url}: {e}")

# def web_crawler(url, depth=2):
#     if depth <= 0 or url in visited_links:
#         return

#     visited_links.add(url)
#     title, text, campus_locations, courses = extract_data(url)
#     if title and text and url not in visited_links:
#         output_data.append([title, text, campus_locations, courses])

#     try:
#         response = requests.get(url)
#         if response.status_code == 200:
#             soup = BeautifulSoup(response.content, 'html.parser')
#             links = soup.find_all('a', href=True)

#             for link in links:
#                 next_url = link['href']
#                 if next_url.startswith('/'):
#                     next_url = base_url + next_url
#                 web_crawler(next_url, depth - 1)
#     except Exception as e:
#         print(f"Error accessing {url}: {e}")

def web_crawler(url, total_limit=float('inf'), per_page_limit=10, depth=2):
    if depth <= 0 or url in visited_links or len(visited_links) >= total_limit:
        return

    print("Scraping:", url)  # Display the URL being accessed

    cleaned_url = clean_url(url)
    visited_links.add(cleaned_url)
    title, text = extract_data(cleaned_url)
    if title and text and text not in [data[1] for data in output_data]:
        output_data.append([title, text])

    try:
        response = requests.get(url)
        if response.status_code == 200 and len(visited_links) < total_limit:
            soup = BeautifulSoup(response.content, 'html.parser')
            links = soup.find_all('a', href=True)

            for link in links:
                next_url = urljoin(url, link['href'])
                if next_url.startswith('/'):
                    next_url = urljoin(base_url, link['href'])
                if next_url not in visited_links and len(visited_links) < total_limit:
                    web_crawler(next_url, total_limit, per_page_limit, depth - 1)
    except Exception as e:
        print(f"Error accessing {url}: {e}")

starting_url = 'https://www.nu.edu.pk/'
total_limit = float('inf')  # Set the total number of links to be scraped to a large value
per_page_limit = 10  # Set the maximum number of links to be scraped from each page
web_crawler(starting_url, total_limit, per_page_limit)

# Display the dataset
for item in output_data:
    title, text = item
    print("Title:", title)
    print("Text:", text)
    print("-" * 50)

# Export data to CSV
with open('output.csv', 'w', newline='', encoding='utf-8') as csvfile:
    csv_writer = csv.writer(csvfile)
    csv_writer.writerow(['Title', 'Text'])
    csv_writer.writerows(output_data)